{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f26f02c",
   "metadata": {},
   "source": [
    "# 主题模型\n",
    "一篇文章 <==> 一个中心思想 <==> 高频词\n",
    "\n",
    "比如一篇文章是讲狗的，“狗”、“骨头”更多的出现在关于狗的文档中。\n",
    "\n",
    "在主题模型中，“主题”通常被定义为一系列语义相关的词。Latent Dirichlet Allocation（LDA）是其中最具代表性的模型。下表中给出了LDA的部分训练结果。其中，每列词是一个主题，每个词后的数值表示该词在主题中的重要程度。\n",
    "\n",
    "![简介](https://github.com/baidu/Familia/wiki/img/table1.png)\n",
    "\n",
    "由于LDA中采用文档内的Bag-of-Words假设，词与词之间的位置信息是被忽略的。在很多工业界场景中，我们往往需要限制某些位置相近的词产生自同一主题，通过SentenceLDA能很好地满足这个需求。SentenceLDA假设同一个句子里的词产生自同一主题，对句子内的词进行了进一步的建模，能捕捉到更加细粒度的共现关系。LDA产生的主题往往被高频词占据，这种现象导致低频词在实际应用中的作用非常有限。Topical Word Embedding (TWE) 利用LDA训练获得的主题为词向量的训练提供补充信息，进而得到词和主题的向量表示。有鉴于向量表示可以较好地建模低频词的语义信息，通过利用词和主题的向量表示，我们可以更好地捕捉每个主题下的低频词的语义信息，提升下游应用的效果。\n",
    "\n",
    "主题模型在工业界的应用范式可以分为两类：语义表示和语义匹配。我们选取一些成功的应用案例加以介绍。\n",
    "\n",
    "+ LDA 主题模型 （文本分类任务）\n",
    "（底层数学推导） ---> \n",
    "涉及的数学：二项分布、Gamma函数、Beta分布、多项分布、Dirichlet分布、马尔科夫链、MCMC、Gibs Sampling、EM算法等等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d981fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f27c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2bd696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
